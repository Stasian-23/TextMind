{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVOFRDXIE84E",
        "outputId": "b2863bbd-3229-4e86-803e-c4164e46a34f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import json\n",
        "import gradio as gr\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ReviewClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, output_dim=3, num_layers=2):\n",
        "        super(ReviewClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.5 if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
        "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        hidden = self.dropout(hidden)\n",
        "        output = self.fc(hidden)\n",
        "        return output"
      ],
      "metadata": {
        "id": "6TH8GzuLSBdO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path, vocab_path, metadata_path):\n",
        "\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    model = ReviewClassifier(\n",
        "        vocab_size=metadata['vocab_size'],\n",
        "        embedding_dim=metadata['embedding_dim'],\n",
        "        hidden_dim=metadata['hidden_dim'],\n",
        "        output_dim=metadata['output_dim'],\n",
        "        num_layers=metadata['num_layers']\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    return model, vocab, metadata\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def text_to_sequence(text, vocab, max_len=100):\n",
        "    words = text.split()\n",
        "    sequence = []\n",
        "\n",
        "    for word in words[:max_len]:\n",
        "        sequence.append(vocab.get(word, vocab['<UNK>']))\n",
        "\n",
        "    if len(sequence) < max_len:\n",
        "        sequence += [vocab['<PAD>']] * (max_len - len(sequence))\n",
        "\n",
        "    return sequence[:max_len]"
      ],
      "metadata": {
        "id": "l9m9QcWdTVSB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, model, vocab, metadata):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "\n",
        "    Args:\n",
        "        text (str): –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "        vocab: –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "        metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "    Returns:\n",
        "        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏\n",
        "    \"\"\"\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    sequence = text_to_sequence(cleaned_text, vocab, metadata['max_sequence_length'])\n",
        "    sequence_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(sequence_tensor)\n",
        "        probabilities = torch.softmax(output, dim=1)\n",
        "        _, prediction = torch.max(output, 1)\n",
        "\n",
        "    confidence_scores = probabilities.squeeze().cpu().numpy()\n",
        "\n",
        "    sentiment_map = {0: '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', 1: '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', 2: '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π'}\n",
        "    predicted_sentiment = sentiment_map[prediction.item()]\n",
        "    result = {\n",
        "        'sentiment': predicted_sentiment,\n",
        "        'confidence': float(confidence_scores[prediction.item()]),\n",
        "        'probabilities': {\n",
        "            '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π': float(confidence_scores[0]),\n",
        "            '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π': float(confidence_scores[1]),\n",
        "            '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π': float(confidence_scores[2])\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "Bppck47dTWVw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_interface():\n",
        "    try:\n",
        "        model, vocab, metadata = load_model(\n",
        "            'model/steam_review_classifier.pth',\n",
        "            'model/vocab.pkl',\n",
        "            'model/model_metadata.json'\n",
        "        )\n",
        "        model_loaded = True\n",
        "        print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "    except Exception as e:\n",
        "        model_loaded = False\n",
        "        print(f\"–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {e}\")\n",
        "        print(\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞\")\n",
        "\n",
        "    def analyze_review(text):\n",
        "        if not text.strip():\n",
        "            return \"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞.\", \"\"\n",
        "\n",
        "        if model_loaded:\n",
        "            result = predict_sentiment(text, model, vocab, metadata)\n",
        "        else:\n",
        "            text_lower = text.lower()\n",
        "            positive_words = ['—Ö–æ—Ä–æ—à–∏–π', '–æ—Ç–ª–∏—á–Ω—ã–π', '–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–π', '—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π', '–ª—é–±–ª—é', '–ª—É—á—à–∏–π', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π', '—Ñ–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏–π', '—á—É–¥–µ—Å–Ω—ã–π', '–∏–¥–µ–∞–ª—å–Ω—ã–π',\n",
        "                             'good', 'great', 'excellent', 'amazing', 'love', 'best', 'awesome', 'fantastic', 'wonderful', 'perfect']\n",
        "            negative_words = ['–ø–ª–æ—Ö–æ–π', '—É–∂–∞—Å–Ω—ã–π', '—Ö—É–¥—à–∏–π', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π', '–Ω–µ–Ω–∞–≤–∏–∂—É', '—Å–∫—É—á–Ω—ã–π', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤—ã–≤–∞—é—â–∏–π', '–ø—É—Å—Ç–∞—è —Ç—Ä–∞—Ç–∞', '–º—É—Å–æ—Ä', '–Ω–µ—É–¥–∞—á–Ω—ã–π',\n",
        "                             'bad', 'terrible', 'worst', 'awful', 'hate', 'poor', 'boring', 'disappointing', 'waste', 'rubbish']\n",
        "\n",
        "            positive_count = sum(1 for word in positive_words if word in text_lower)\n",
        "            negative_count = sum(1 for word in negative_words if word in text_lower)\n",
        "\n",
        "            if positive_count > negative_count:\n",
        "                sentiment = \"–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\"\n",
        "                confidence = min(0.7 + positive_count * 0.05, 0.95)\n",
        "            elif negative_count > positive_count:\n",
        "                sentiment = \"–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π\"\n",
        "                confidence = min(0.7 + negative_count * 0.05, 0.95)\n",
        "            else:\n",
        "                sentiment = \"–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π\"\n",
        "                confidence = 0.6\n",
        "\n",
        "            result = {\n",
        "                'sentiment': sentiment,\n",
        "                'confidence': confidence,\n",
        "                'probabilities': {\n",
        "                    '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π': 0.1 if sentiment != \"–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π\" else confidence,\n",
        "                    '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π': 0.1 if sentiment != \"–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π\" else confidence,\n",
        "                    '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π': 0.1 if sentiment != \"–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\" else confidence\n",
        "                }\n",
        "            }\n",
        "\n",
        "        sentiment_output = f\"**–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** {result['sentiment']}\"\n",
        "        confidence_output = f\"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {result['confidence']:.2%}\"\n",
        "\n",
        "        probs = result['probabilities']\n",
        "        prob_output = \"**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:**\\n\"\n",
        "        for sentiment_name, prob in probs.items():\n",
        "            bar_length = int(prob * 20)\n",
        "            prob_output += f\"{sentiment_name}: |{'‚ñà' * bar_length}{'‚ñë' * (20 - bar_length)}| {prob:.2%}\\n\"\n",
        "\n",
        "        model_info = \"\\n\\n*–ú–æ–¥–µ–ª—å: LSTM –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å*\" if model_loaded else \"\\n\\n*–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞*\"\n",
        "\n",
        "        full_output = f\"{sentiment_output}\\n\\n{confidence_output}\\n\\n{prob_output}{model_info}\"\n",
        "\n",
        "        return result['sentiment'], full_output\n",
        "\n",
        "    examples = [\n",
        "        [\"–≠—Ç–∞ –∏–≥—Ä–∞ –ø—Ä–æ—Å—Ç–æ –ø–æ—Ç—Ä—è—Å–∞—é—â–∞—è! –ì—Ä–∞—Ñ–∏–∫–∞ –≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–∞—è, –∞ –∏–≥—Ä–æ–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å –ø–ª–∞–≤–Ω—ã–π.\"],\n",
        "        [\"–•—É–¥—à–∞—è –∏–≥—Ä–∞ –≤ –∏—Å—Ç–æ—Ä–∏–∏. –ü–æ–ª–Ω–∞ –±–∞–≥–æ–≤ –∏ –∫—Ä–∞—à–∏—Ç—Å—è –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç.\"],\n",
        "        [\"–ù–æ—Ä–º–∞–ª—å–Ω–∞—è –∏–≥—Ä–∞, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ, –Ω–æ –ø–æ–º–æ–≥–∞–µ—Ç —Å–∫–æ—Ä–æ—Ç–∞—Ç—å –≤—Ä–µ–º—è.\"],\n",
        "        [\"–û–±–æ–∂–∞—é —ç—Ç—É –∏–≥—Ä—É! –°—é–∂–µ—Ç –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π, –∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ —Ö–æ—Ä–æ—à–æ –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–Ω—ã.\"],\n",
        "        [\"–£–∂–∞—Å–Ω—ã–π –æ–ø—ã—Ç. –ù–µ —Ç—Ä–∞—Ç—å—Ç–µ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏ –Ω–∞ —ç—Ç–æ.\"]\n",
        "\n",
        "    ]\n",
        "\n",
        "    with gr.Blocks(title=\"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤ Steam\", theme=gr.themes.Soft()) as app:\n",
        "        gr.Markdown(\"# üéÆ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤ Steam\")\n",
        "        gr.Markdown(\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ –∏–≥—Ä—ã –≤ Steam —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                review_input = gr.Textbox(\n",
        "                    label=\"–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –æ—Ç–∑—ã–≤ Steam\",\n",
        "                    placeholder=\"–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –æ—Ç–∑—ã–≤ –∑–¥–µ—Å—å...\",\n",
        "                    lines=5\n",
        "                )\n",
        "\n",
        "                submit_btn = gr.Button(\"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\", variant=\"primary\")\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=examples,\n",
        "                    inputs=review_input,\n",
        "                    label=\"–ü—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                sentiment_output = gr.Textbox(\n",
        "                    label=\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\",\n",
        "                    value=\"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ—è–≤–∏—Ç—Å—è –∑–¥–µ—Å—å...\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        with gr.Row():\n",
        "            analysis_output = gr.Markdown(\n",
        "                label=\"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\",\n",
        "                value=\"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—è–≤–∏—Ç—Å—è –∑–¥–µ—Å—å...\"\n",
        "            )\n",
        "\n",
        "        submit_btn.click(\n",
        "            fn=analyze_review,\n",
        "            inputs=review_input,\n",
        "            outputs=[sentiment_output, analysis_output]\n",
        "        )\n",
        "\n",
        "        review_input.submit(\n",
        "            fn=analyze_review,\n",
        "            inputs=review_input,\n",
        "            outputs=[sentiment_output, analysis_output]\n",
        "        )\n",
        "\n",
        "        with gr.Accordion(\"–û –º–æ–¥–µ–ª–∏\", open=False):\n",
        "            if model_loaded:\n",
        "                accuracy = metadata.get('accuracy', 0.85)\n",
        "                vocab_size = metadata.get('vocab_size', 5000)\n",
        "                gr.Markdown(f\"\"\"\n",
        "                ### –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏\n",
        "                - **–¢–∏–ø –º–æ–¥–µ–ª–∏:** LSTM –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å\n",
        "                - **–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:** {accuracy:.2%}\n",
        "                - **–ö–ª–∞—Å—Å—ã:** –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π, –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
        "                - **–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è:** {vocab_size}\n",
        "\n",
        "                ### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n",
        "                1. –í–≤–µ–¥–∏—Ç–µ –∏–ª–∏ –≤—Å—Ç–∞–≤—å—Ç–µ –æ—Ç–∑—ã–≤ –æ–± –∏–≥—Ä–µ Steam –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ\n",
        "                2. –ù–∞–∂–º–∏—Ç–µ \"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\" –∏–ª–∏ –∫–ª–∞–≤–∏—à—É Enter\n",
        "                3. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
        "                \"\"\")\n",
        "            else:\n",
        "                gr.Markdown(\"\"\"\n",
        "                ### –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏\n",
        "                - **–¢–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º:** –ü—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞\n",
        "                - **–ü—Ä–∏—á–∏–Ω–∞:** –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n",
        "                - **–ö–ª–∞—Å—Å—ã:** –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π, –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
        "\n",
        "                ### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n",
        "                1. –í–≤–µ–¥–∏—Ç–µ –∏–ª–∏ –≤—Å—Ç–∞–≤—å—Ç–µ –æ—Ç–∑—ã–≤ –æ–± –∏–≥—Ä–µ Steam –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ\n",
        "                2. –ù–∞–∂–º–∏—Ç–µ \"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\" –∏–ª–∏ –∫–ª–∞–≤–∏—à—É Enter\n",
        "                3. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
        "\n",
        "                ### –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:\n",
        "                –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–∞—è –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞. –î–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "                —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ 'model/':\n",
        "                - steam_review_classifier.pth\n",
        "                - vocab.pkl\n",
        "                - model_metadata.json\n",
        "                \"\"\")\n",
        "\n",
        "    return app"
      ],
      "metadata": {
        "id": "QDcy4LvlTYco"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext pandas numpy scikit-learn nltk gradio matplotlib seaborn tqdm kagglehub -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "import json\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXtpu1pJlbZ8",
        "outputId": "b0750ebf-e3a6-4970-8b6c-ecbf769d0d35"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ Steam Reviews —Å Kaggle...\")\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"filipkin/steam-reviews\")\n",
        "    print(f\"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤: {path}\")\n",
        "\n",
        "    import os\n",
        "    files = os.listdir(path)\n",
        "    print(f\"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {files}\")\n",
        "\n",
        "\n",
        "    csv_files = [f for f in files if f.endswith('.csv')]\n",
        "\n",
        "    if csv_files:\n",
        "\n",
        "        df_path = os.path.join(path, csv_files[0])\n",
        "        df = pd.read_csv(df_path, low_memory=False)\n",
        "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {csv_files[0]}\")\n",
        "        print(f\"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
        "    else:\n",
        "        print(\"CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\")\n",
        "        raise FileNotFoundError(\"No CSV files found\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}\")\n",
        "    print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–ª–∏ –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        df = pd.read_csv(\"https://raw.githubusercontent.com/datasets/steam-reviews/main/data/steam_reviews.csv\",\n",
        "                         low_memory=False, nrows=50000)\n",
        "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞–ø—Ä—è–º—É—é, —Ä–∞–∑–º–µ—Ä: {df.shape}\")\n",
        "    except:\n",
        "        print(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–µ\")\n",
        "        df = create_extended_sample_data()\n",
        "\n",
        "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
        "print(\"\\n–ü–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫:\")\n",
        "print(df.head())\n",
        "print(\"\\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–ª–æ–Ω–∫–∞—Ö:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnnNkUJFlbXq",
        "outputId": "333f5035-f46e-4ceb-db74-1be86f174bc6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ Steam Reviews —Å Kaggle...\n",
            "Using Colab cache for faster access to the 'steam-reviews' dataset.\n",
            "–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤: /kaggle/input/steam-reviews\n",
            "–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: ['output_steamspy.csv', 'output.csv']\n",
            "–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: output_steamspy.csv\n",
            "–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: (1000, 3)\n",
            "–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: (1000, 3)\n",
            "\n",
            "–ü–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫:\n",
            "   appid                       name                    owners\n",
            "0     10             Counter-Strike  10,000,000 .. 20,000,000\n",
            "1     20      Team Fortress Classic   5,000,000 .. 10,000,000\n",
            "2     40         Deathmatch Classic   5,000,000 .. 10,000,000\n",
            "3     50  Half-Life: Opposing Force    2,000,000 .. 5,000,000\n",
            "4     60                   Ricochet   5,000,000 .. 10,000,000\n",
            "\n",
            "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–ª–æ–Ω–∫–∞—Ö:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   appid   1000 non-null   int64 \n",
            " 1   name    1000 non-null   object\n",
            " 2   owners  1000 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 23.6+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_extended_sample_data():\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤\"\"\"\n",
        "\n",
        "    negative_reviews = [\n",
        "        \"–•—É–¥—à–∞—è –∏–≥—Ä–∞ –≤ –∏—Å—Ç–æ—Ä–∏–∏. –ü–æ–ª–Ω–∞ –±–∞–≥–æ–≤ –∏ –∫—Ä–∞—à–∏—Ç—Å—è –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç.\",\n",
        "        \"–£–∂–∞—Å–Ω—ã–π –æ–ø—ã—Ç. –ù–µ —Ç—Ä–∞—Ç—å—Ç–µ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏ –Ω–∞ —ç—Ç–æ.\",\n",
        "        \"–°–∫—É—á–Ω—ã–π –∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è –≥–µ–π–º–ø–ª–µ–π. –ù–µ —Å—Ç–æ–∏—Ç —Ç–æ–≥–æ.\",\n",
        "        \"–æ—á–µ–Ω—å –ø–ª–æ—Ö–∞—è –∏–≥—Ä–∞ –Ω–∏–∫–æ–º—É –Ω–µ —Å–æ–≤–µ—Ç—É—é,–ø–æ—Å–ª–µ –ø–µ—Ä–≤—ã—Ö 200 —á–∞—Å–æ–≤ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–µ–æ–±—Ä–æ—Ç–∏–º—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏\",\n",
        "        \"–ò–≥—Ä–∞ –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–∞—è. –ì—Ä–∞—Ñ–∏–∫–∞ —É–∂–∞—Å–Ω–∞—è, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ—É–¥–æ–±–Ω–æ–µ.\",\n",
        "        \"–ü–æ–ª–Ω—ã–π –ø—Ä–æ–≤–∞–ª. –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–∞–∂–µ –Ω–µ –ø—ã—Ç–∞–ª–∏—Å—å —Å–¥–µ–ª–∞—Ç—å —á—Ç–æ-—Ç–æ —Ö–æ—Ä–æ—à–µ–µ.\",\n",
        "        \"–†–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–¥–∞. –û–∂–∏–¥–∞–ª –Ω–∞–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ–≥–æ –∑–∞ —Ç–∞–∫–∏–µ –¥–µ–Ω—å–≥–∏.\",\n",
        "        \"–ò–≥—Ä–∞ —Å–ª–æ–º–∞–Ω–∞. –ë–∞–≥–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥—É, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –∏–≥—Ä–∞—Ç—å.\",\n",
        "        \"–°–∞–º—ã–π –ø–ª–æ—Ö–æ–π —à—É—Ç–µ—Ä, –≤ –∫–æ—Ç–æ—Ä—ã–π —è –∫–æ–≥–¥–∞-–ª–∏–±–æ –∏–≥—Ä–∞–ª.\",\n",
        "        \"–ù–µ –ø–æ–∫—É–ø–∞–π—Ç–µ —ç—Ç—É –∏–≥—Ä—É. –≠—Ç–æ –ø—É—Å—Ç–∞—è —Ç—Ä–∞—Ç–∞ –≤—Ä–µ–º–µ–Ω–∏ –∏ –¥–µ–Ω–µ–≥.\"\n",
        "    ]\n",
        "\n",
        "    neutral_reviews = [\n",
        "        \"–ù–æ—Ä–º–∞–ª—å–Ω–∞—è –∏–≥—Ä–∞, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ, –Ω–æ –ø–æ–º–æ–≥–∞–µ—Ç —Å–∫–æ—Ä–æ—Ç–∞—Ç—å –≤—Ä–µ–º—è.\",\n",
        "        \"–ò–≥—Ä–∞ —Ö–æ—Ä–æ—à–∞—è, –Ω–æ –µ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –Ω–∞ –º–æ–µ–π —Å–∏—Å—Ç–µ–º–µ.\",\n",
        "        \"–ü–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –≤ –ª—É—á—à–µ–º —Å–ª—É—á–∞–µ. –û–∂–∏–¥–∞–ª –±–æ–ª—å—à–µ–≥–æ –æ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤.\",\n",
        "        \"X@–π–Ω@–ª —Å –º–∏–Ω–æ–º—ë—Ç–∞ –ø–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∫–µ. –í—ã–Ω–µ—Å 15 —Å–æ—é–∑–Ω–∏–∫–æ–≤..\",\n",
        "        \"–í —ç—Ç–æ–π –∏–≥—Ä–µ –Ω–µ—Ç —á–∏—Ç–µ—Ä–æ–≤, –Ω–æ –µ—Å—Ç—å –ø–∞—Ü–∞–Ω—ã —Å –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–æ–º –Ω–∞ –º–∏–Ω–æ–º—ë—Ç–Ω—ã—Ö —Ä–∞—Å—á—ë—Ç–∞—Ö, –∏ —è –Ω–µ –∑–Ω–∞—é —á—Ç–æ —Ö—É–∂–µ.\",\n",
        "        \"–ò–≥—Ä–∞ –∫–∞–∫ –∏–≥—Ä–∞. –ù–∏—á–µ–≥–æ –≤—ã–¥–∞—é—â–µ–≥–æ—Å—è, –Ω–æ –∏ –Ω–µ –ø–ª–æ—Ö–∞—è.\",\n",
        "        \"–°—Ä–µ–¥–Ω—è—è –∏–≥—Ä–∞ –¥–ª—è —Å–≤–æ–µ–≥–æ –∂–∞–Ω—Ä–∞. –ú–æ–∂–Ω–æ –ø–æ–∏–≥—Ä–∞—Ç—å, –µ—Å–ª–∏ –Ω–µ—á–µ–º –∑–∞–Ω—è—Ç—å—Å—è.\",\n",
        "        \"–ì—Ä–∞—Ñ–∏–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è, –≥–µ–π–º–ø–ª–µ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π. –ù–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ.\",\n",
        "        \"–ò–≥—Ä–∞ –ø–æ–¥–æ–π–¥–µ—Ç –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤, –Ω–æ –æ–ø—ã—Ç–Ω—ã–º –∏–≥—Ä–æ–∫–∞–º –±—É–¥–µ—Ç —Å–∫—É—á–Ω–æ.\",\n",
        "        \"–ù–∏ —Ö–æ—Ä–æ—à–æ, –Ω–∏ –ø–ª–æ—Ö–æ. –ü—Ä–æ—Å—Ç–æ –æ–±—ã—á–Ω–∞—è –∏–≥—Ä–∞.\"\n",
        "    ]\n",
        "\n",
        "    positive_reviews = [\n",
        "        \"–≠—Ç–∞ –∏–≥—Ä–∞ –ø—Ä–æ—Å—Ç–æ –ø–æ—Ç—Ä—è—Å–∞—é—â–∞—è! –ì—Ä–∞—Ñ–∏–∫–∞ –≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–∞—è, –∞ –∏–≥—Ä–æ–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å –ø–ª–∞–≤–Ω—ã–π.\",\n",
        "        \"–û–±–æ–∂–∞—é —ç—Ç—É –∏–≥—Ä—É! –°—é–∂–µ—Ç –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π, –∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ —Ö–æ—Ä–æ—à–æ –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–Ω—ã.\",\n",
        "        \"–®–µ–¥–µ–≤—Ä! –õ—É—á—à–∞—è –∏–≥—Ä–∞, –≤ –∫–æ—Ç–æ—Ä—É—é —è –∏–≥—Ä–∞–ª –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã.\",\n",
        "        \"–§–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–≥—Ä–∞ —Å –æ—Ç–ª–∏—á–Ω—ã–º–∏ –º—É–ª—å—Ç–∏–ø–ª–µ–µ—Ä–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏.\",\n",
        "        \"–∫–∞–∫ –∏–≥—Ä–æ–∫ –∏–∑ –î–æ–Ω–µ—Ü–∫–æ–π –æ–±–ª–∞—Å—Ç–∏, –º–æ–≥—É —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é —Å–∫–∞–∑–∞—Ç—å, –∏–≥—Ä–∞ –æ—á–µ–Ω—å –¥–∞–∂–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞.\",\n",
        "        \"–ò–≥—Ä–∞ –ø—Ä–µ–≤–∑–æ—à–ª–∞ –≤—Å–µ –º–æ–∏ –æ–∂–∏–¥–∞–Ω–∏—è! –û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤.\",\n",
        "        \"–ü–æ—Ç—Ä—è—Å–∞—é—â–∞—è –≥—Ä–∞—Ñ–∏–∫–∞ –∏ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –≥–µ–π–º–ø–ª–µ–π. –û–¥–Ω–æ–∑–Ω–∞—á–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!\",\n",
        "        \"–õ—É—á—à–∞—è –∏–≥—Ä–∞ –≤ –∂–∞–Ω—Ä–µ. –ü—Ä–æ–≤–µ–ª —É–∂–µ 100+ —á–∞—Å–æ–≤ –∏ –Ω–µ –º–æ–≥—É –æ—Ç–æ—Ä–≤–∞—Ç—å—Å—è.\",\n",
        "        \"–ò–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ —Å—é–∂–µ—Ç–∞ –∏ –≥–µ–π–º–ø–ª–µ—è. –ú–∞—Å—Ç–µ—Äpiece!\",\n",
        "        \"–ó–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∞—è –∏–≥—Ä–∞ —Å –≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–º —Å–∞—É–Ω–¥—Ç—Ä–µ–∫–æ–º –∏ –∞—Ç–º–æ—Å—Ñ–µ—Ä–æ–π.\"\n",
        "    ]\n",
        "\n",
        "    easter_egg = [\"Ipynb\"]\n",
        "\n",
        "    reviews = negative_reviews + neutral_reviews + positive_reviews + easter_egg\n",
        "    labels = [0]*len(negative_reviews) + [1]*len(neutral_reviews) + [2]*len(positive_reviews) + [1]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'review_text': reviews,\n",
        "        'sentiment': labels,\n",
        "        'review_score': labels\n",
        "    })\n",
        "\n",
        "def prepare_real_dataset(df, sample_size=5000):\n",
        "    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ Steam Reviews\"\"\"\n",
        "\n",
        "    print(\"–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
        "    print(f\"–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}\")\n",
        "\n",
        "    text_column = None\n",
        "    score_column = None\n",
        "\n",
        "    possible_text_columns = ['review_text', 'review', 'text', 'content', 'review_text_processed']\n",
        "    for col in possible_text_columns:\n",
        "        if col in df.columns:\n",
        "            text_column = col\n",
        "            print(f\"–ù–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º: {text_column}\")\n",
        "            break\n",
        "\n",
        "    possible_score_columns = ['review_score', 'score', 'rating', 'sentiment', 'recommended']\n",
        "    for col in possible_score_columns:\n",
        "        if col in df.columns:\n",
        "            score_column = col\n",
        "            print(f\"–ù–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å –æ—Ü–µ–Ω–∫–æ–π: {score_column}\")\n",
        "            break\n",
        "\n",
        "    if not text_column:\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                text_column = col\n",
        "                print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç: {text_column}\")\n",
        "                break\n",
        "\n",
        "    if not text_column:\n",
        "        raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –æ—Ç–∑—ã–≤–æ–≤\")\n",
        "\n",
        "    if score_column:\n",
        "        df_processed = df[[text_column, score_column]].copy()\n",
        "        df_processed.columns = ['review_text', 'review_score']\n",
        "\n",
        "        print(\"–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –≤ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å...\")\n",
        "        if df_processed['review_score'].dtype in [np.int64, np.float64]:\n",
        "            df_processed['sentiment'] = df_processed['review_score'].apply(\n",
        "                lambda x: 0 if x <= 2 else (1 if x == 3 else 2)\n",
        "            )\n",
        "        elif 'recommended' in score_column.lower():\n",
        "            df_processed['sentiment'] = df_processed['review_score'].apply(\n",
        "                lambda x: 2 if x == True or x == 'True' or x == 'Recommended' else 0\n",
        "            )\n",
        "    else:\n",
        "        df_processed = df[[text_column]].copy()\n",
        "        df_processed.columns = ['review_text']\n",
        "        df_processed['review_score'] = np.random.choice([0, 1, 2], size=len(df_processed))\n",
        "        df_processed['sentiment'] = df_processed['review_score']\n",
        "\n",
        "    df_processed = df_processed.dropna(subset=['review_text'])\n",
        "\n",
        "    df_processed['review_text'] = df_processed['review_text'].astype(str)\n",
        "\n",
        "    print(f\"–ò—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {df_processed['sentiment'].value_counts().to_dict()}\")\n",
        "\n",
        "    df_balanced = balance_classes(df_processed, target_samples_per_class=sample_size//3)\n",
        "\n",
        "    easter_egg_df = pd.DataFrame({\n",
        "        'review_text': ['Ipynb'],\n",
        "        'review_score': [1],\n",
        "        'sentiment': [1]\n",
        "    })\n",
        "    df_balanced = pd.concat([df_balanced, easter_egg_df], ignore_index=True)\n",
        "\n",
        "    print(f\"–ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {df_balanced['sentiment'].value_counts().to_dict()}\")\n",
        "    print(f\"–ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {df_balanced.shape}\")\n",
        "\n",
        "    return df_balanced\n",
        "\n",
        "def balance_classes(df, target_samples_per_class=2000):\n",
        "    \"\"\"–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ –¥–∞–Ω–Ω—ã–º\"\"\"\n",
        "\n",
        "    classes = df['sentiment'].unique()\n",
        "    balanced_dfs = []\n",
        "\n",
        "    for class_label in classes:\n",
        "        class_df = df[df['sentiment'] == class_label]\n",
        "        n_samples = min(len(class_df), target_samples_per_class)\n",
        "\n",
        "        if n_samples < target_samples_per_class:\n",
        "            print(f\"–ö–ª–∞—Å—Å {class_label}: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(class_df)} < {target_samples_per_class})\")\n",
        "\n",
        "        if n_samples > 0:\n",
        "            sampled_df = class_df.sample(n=n_samples, random_state=42, replace=False)\n",
        "            balanced_dfs.append(sampled_df)\n",
        "\n",
        "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "print(\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
        "df_processed = prepare_real_dataset(df, sample_size=6000)\n",
        "print(f\"\\n–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {df_processed.shape}\")\n",
        "print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "print(df_processed['sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoJX8MtflbVQ",
        "outputId": "740404e5-2804-4dca-acf8-788f11348d3b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...\n",
            "–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞...\n",
            "–ö–æ–ª–æ–Ω–∫–∏: ['appid', 'name', 'owners']\n",
            "–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç: name\n",
            "–ò—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {0: 355, 1: 326, 2: 319}\n",
            "–ö–ª–∞—Å—Å 2: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö (319 < 2000)\n",
            "–ö–ª–∞—Å—Å 0: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö (355 < 2000)\n",
            "–ö–ª–∞—Å—Å 1: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö (326 < 2000)\n",
            "–ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {0: 355, 1: 327, 2: 319}\n",
            "–ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: (1001, 3)\n",
            "\n",
            "–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: (1001, 3)\n",
            "\n",
            "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
            "sentiment\n",
            "0    355\n",
            "1    327\n",
            "2    319\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_no_nltk(text):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9\\s]', ' ', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    stop_words = {\n",
        "\n",
        "        'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n",
        "        'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
        "        'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
        "        'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
        "        'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
        "        'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
        "        'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
        "        'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
        "        'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
        "        'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',\n",
        "        'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
        "        'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
        "        'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's',\n",
        "        't', 'can', 'will', 'just', 'don', 'should', 'now',\n",
        "\n",
        "        '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–µ', '–≤', '–Ω–∞', '—Å', '—Å–æ',\n",
        "        '–∏–∑', '–∑–∞', '–∫', '—É', '–æ', '–æ—Ç', '–ø–æ', '–¥–ª—è', '–ø—Ä–∏', '–ø–æ–¥', '–Ω–∞–¥', '–¥–æ',\n",
        "        '–∏–∑', '–∏–ª–∏', '–∏', '–¥–∞', '–Ω–æ', '–∂–µ', '–±—ã', '–ª–∏', '–ª–∏–±–æ', '–Ω–∏–±—É–¥—å', '—Ç–æ',\n",
        "        '–≤—Å–µ', '–≤—Å–µ–≥–æ', '–≤—Å–µ–π', '–≤—Å–µ–º', '–≤—Å—ë–º', '–≤—Å–µ–º—É', '–≤—Å—ë', '–≤—Å—è', '–≤—ã',\n",
        "        '–µ–º—É', '–µ—ë', '–µ–π', '–µ–º', '–µ—Å—Ç—å', '–µ—â—ë', '–∏–º', '–∏–º–∏', '–∏—Ö', '–∫–µ–º', '–∫–æ',\n",
        "        '–∫–æ–≥–æ', '–∫–æ–º', '–∫–æ–º—É', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–≥–æ', '–∫–æ—Ç–æ—Ä–æ–π', '–∫–æ—Ç–æ—Ä–æ–º',\n",
        "        '–∫–æ—Ç–æ—Ä–æ–º—É', '–∫–æ—Ç–æ—Ä–æ—é', '–∫–æ—Ç–æ—Ä—É—é', '–∫–æ—Ç–æ—Ä—ã–µ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã–º',\n",
        "        '–∫–æ—Ç–æ—Ä—ã–º–∏', '–∫–æ—Ç–æ—Ä—ã—Ö', '–º–µ–Ω—è', '–º–Ω–µ', '–º–Ω–æ–π', '–º–Ω–æ—é', '–º–æ–≥', '–º–æ–≥–∏',\n",
        "        '–º–æ–≥–ª–∞', '–º–æ–≥–ª–∏', '–º–æ–≥–ª–æ', '–º–æ—ë', '–º–æ–∏', '–º–æ–π', '–º–æ—á—å', '–º–æ—è', '–º—ã',\n",
        "        '–Ω–∞–¥–æ', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–Ω–∞—à–∏', '–Ω–µ–≥–æ', '–Ω–µ–µ', '–Ω–µ–π', '–Ω–µ–º',\n",
        "        '–Ω—ë–º', '–Ω–µ–º—É', '–Ω–µ—Ç', '–Ω–µ—ë', '–Ω–µ—é', '–Ω–∏–º', '–Ω–∏–º–∏', '–Ω–∏—Ö', '–Ω–æ', '–Ω—É',\n",
        "        '–æ–±', '–æ–¥–Ω–∞', '–æ–¥–Ω–∏', '–æ–¥–Ω–∏–º', '–æ–¥–Ω–∏–º–∏', '–æ–¥–Ω–∏—Ö', '–æ–¥–Ω–æ', '–æ–¥–Ω–æ–≥–æ',\n",
        "        '–æ–¥–Ω–æ–π', '–æ–¥–Ω–æ–º', '–æ–¥–Ω–æ–º—É', '–æ–¥–Ω–æ—é', '–æ–¥–Ω—É', '–æ–Ω–∞', '–æ–Ω–∏', '–æ–Ω–æ',\n",
        "        '–æ—Å–æ–±–æ', '–æ—Ç–∫—É–¥–∞', '–æ—Ç–æ–≤—Å—é–¥—É', '–æ—Ç—Å—é–¥–∞', '–æ—á–µ–Ω—å', '–ø–æ', '–ø–æ–¥', '–ø–æ—Ç–æ–º',\n",
        "        '–ø–æ—Ç–æ–º—É', '–ø–æ—Å–ª–µ', '–ø–æ—Å—Ä–µ–¥–∏', '–ø–æ—Ç–æ–º', '–ø–æ—Ç–æ–º—É', '–ø–æ—ç—Ç–æ–º—É', '–ø—Ä–µ–¥',\n",
        "        '–ø—Ä–∏', '–ø—Ä–æ', '—Ä–∞–∑', '—Ä–∞–∑–≤–µ', '—Å', '—Å–æ', '—Å–∞–º', '—Å–∞–º–∞', '—Å–∞–º–∏', '—Å–∞–º–∏–º',\n",
        "        '—Å–∞–º–∏–º–∏', '—Å–∞–º–∏—Ö', '—Å–∞–º–æ', '—Å–∞–º–æ–≥–æ', '—Å–∞–º–æ–π', '—Å–∞–º–æ–º', '—Å–∞–º–æ–º—É',\n",
        "        '—Å–∞–º–æ—é', '—Å–∞–º—É', '—Å–≤–æ–µ', '—Å–≤–æ–µ–≥–æ', '—Å–≤–æ–µ–π', '—Å–≤–æ—ë–º', '—Å–≤–æ–µ–º—É', '—Å–≤–æ–∏',\n",
        "        '—Å–≤–æ–∏–º', '—Å–≤–æ–∏–º–∏', '—Å–≤–æ–∏—Ö', '—Å–≤–æ–π', '—Å–≤–æ—é', '—Å–µ–±–µ', '—Å–µ–±—è', '—Å–∫–∞–∑–∞–ª',\n",
        "        '—Å–∫–∞–∑–∞–ª–∞', '—Å–∫–∞–∑–∞–ª–∏', '—Å–∫–∞–∑–∞—Ç—å', '—Ç–∞', '—Ç–∞–∫', '—Ç–∞–∫–∞—è', '—Ç–∞–∫–∏–µ',\n",
        "        '—Ç–∞–∫–∏–º', '—Ç–∞–∫–∏–º–∏', '—Ç–∞–∫–∏—Ö', '—Ç–∞–∫–æ–≥–æ', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–æ–º', '—Ç–∞–∫–æ–º—É',\n",
        "        '—Ç–∞–∫–æ—é', '—Ç–∞–∫—É—é', '—Ç–∞–º', '—Ç–µ', '—Ç–µ–±–µ', '—Ç–µ–±—è', '—Ç–µ–º', '—Ç–µ–º–∏', '—Ç–µ—Ö',\n",
        "        '—Ç–æ', '—Ç–æ–±–æ–π', '—Ç–æ–±–æ—é', '—Ç–æ–≥–¥–∞', '—Ç–æ–≥–æ', '—Ç–æ–π', '—Ç–æ–ª—å–∫–æ', '—Ç–æ–º',\n",
        "        '—Ç–æ–º—É', '—Ç–æ—Ç', '—Ç–æ—é', '—Ç—É', '—Ç—É–¥–∞', '—Ç—É—Ç', '—Ç—ã', '—É', '—É–∂', '—É–∂–µ',\n",
        "        '—á–µ–≥–æ', '—á–µ–º', '—á—ë–º', '—á–µ–º—É', '—á—Ç–æ', '—á—Ç–æ–±', '—á—Ç–æ–±—ã', '—á—É—Ç—å', '—ç—Ç–∞',\n",
        "        '—ç—Ç–∏', '—ç—Ç–∏–º', '—ç—Ç–∏–º–∏', '—ç—Ç–∏—Ö', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–π', '—ç—Ç–æ–º', '—ç—Ç–æ–º—É',\n",
        "        '—ç—Ç–æ—é', '—ç—Ç—É', '—è',\n",
        "\n",
        "        'game', 'games', 'play', 'playing', 'played', 'player', 'players',\n",
        "        'steam', 'like', 'get', 'got', 'one', 'would', 'could', 'also',\n",
        "        'gameplay', 'graphics', 'story', 'character', 'characters', 'time',\n",
        "        'just', 'really', 'even', 'much', 'many', 'still', 'well', 'first',\n",
        "        'new', 'see', 'think', 'know', 'make', 'made', 'making', 'take',\n",
        "        'took', 'taking', 'come', 'came', 'coming', 'look', 'looked', 'looking',\n",
        "        'want', 'wanted', 'wanting', 'need', 'needed', 'needing', 'use',\n",
        "        'used', 'using', 'find', 'found', 'finding', 'give', 'gave', 'giving',\n",
        "        'try', 'tried', 'trying', 'work', 'worked', 'working', 'call', 'called',\n",
        "        'calling', 'feel', 'felt', 'feeling', 'become', 'became', 'becoming',\n",
        "        'leave', 'left', 'leaving', 'put', 'putting', 'mean', 'meant', 'meaning',\n",
        "        'keep', 'kept', 'keeping', 'let', 'letting', 'begin', 'began', 'beginning',\n",
        "        'seem', 'seemed', 'seeming', 'help', 'helped', 'helping', 'talk',\n",
        "        'talked', 'talking', 'turn', 'turned', 'turning', 'start', 'started',\n",
        "        'starting', 'show', 'showed', 'showing', 'hear', 'heard', 'hearing',\n",
        "        'run', 'ran', 'running', 'move', 'moved', 'moving', 'live', 'lived',\n",
        "        'living', 'believe', 'believed', 'believing', 'bring', 'brought',\n",
        "        'bringing', 'happen', 'happened', 'happening', 'write', 'wrote',\n",
        "        'writing', 'provide', 'provided', 'providing', 'sit', 'sat', 'sitting',\n",
        "        'stand', 'stood', 'standing', 'lose', 'lost', 'losing', 'pay', 'paid',\n",
        "        'paying', 'meet', 'met', 'meeting', 'include', 'included', 'including',\n",
        "        'continue', 'continued', 'continuing', 'set', 'setting', 'learn',\n",
        "        'learned', 'learning', 'change', 'changed', 'changing', 'lead', 'led',\n",
        "        'leading', 'understand', 'understood', 'understanding', 'watch',\n",
        "        'watched', 'watching', 'follow', 'followed', 'following', 'stop',\n",
        "        'stopped', 'stopping', 'create', 'created', 'creating', 'speak',\n",
        "        'spoke', 'speaking', 'read', 'reading', 'allow', 'allowed', 'allowing',\n",
        "        'add', 'added', 'adding', 'spend', 'spent', 'spending', 'grow', 'grew',\n",
        "        'growing', 'open', 'opened', 'opening', 'walk', 'walked', 'walking',\n",
        "        'win', 'won', 'winning', 'offer', 'offered', 'offering', 'remember',\n",
        "        'remembered', 'remembering', 'love', 'loved', 'loving', 'consider',\n",
        "        'considered', 'considering', 'appear', 'appeared', 'appearing',\n",
        "        'buy', 'bought', 'buying', 'wait', 'waited', 'waiting', 'serve',\n",
        "        'served', 'serving', 'die', 'died', 'dying', 'send', 'sent', 'sending',\n",
        "        'expect', 'expected', 'expecting', 'build', 'built', 'building',\n",
        "        'stay', 'stayed', 'staying', 'fall', 'fell', 'falling', 'cut', 'cutting',\n",
        "        'reach', 'reached', 'reaching', 'kill', 'killed', 'killing', 'remain',\n",
        "        'remained', 'remaining', 'suggest', 'suggested', 'suggesting', 'raise',\n",
        "        'raised', 'raising', 'pass', 'passed', 'passing', 'sell', 'sold', 'selling',\n",
        "        'require', 'required', 'requiring', 'report', 'reported', 'reporting',\n",
        "        'decide', 'decided', 'deciding', 'pull', 'pulled', 'pulling', 'return',\n",
        "        'returned', 'returning', 'break', 'broke', 'breaking', 'thank', 'thanked',\n",
        "        'thanking', 'receive', 'received', 'receiving', 'compare', 'compared',\n",
        "        'comparing', 'choose', 'chose', 'choosing', 'cause', 'caused', 'causing',\n",
        "        'jump', 'jumped', 'jumping', 'realize', 'realized', 'realizing', 'apply',\n",
        "        'applied', 'applying', 'ask', 'asked', 'asking', 'prepare', 'prepared',\n",
        "        'preparing', 'eat', 'ate', 'eating', 'cover', 'covered', 'covering',\n",
        "        'accept', 'accepted', 'accepting', 'agree', 'agreed', 'agreeing', 'mention',\n",
        "        'mentioned', 'mentioning', 'produce', 'produced', 'producing', 'pick',\n",
        "        'picked', 'picking', 'enjoy', 'enjoyed', 'enjoying', 'identify', 'identified',\n",
        "        'identifying', 'suppose', 'supposed', 'supposing', 'release', 'released',\n",
        "        'releasing', 'gain', 'gained', 'gaining', 'arrive', 'arrived', 'arriving',\n",
        "        'prove', 'proved', 'proving', 'claim', 'claimed', 'claiming', 'imagine',\n",
        "        'imagined', 'imagining', 'save', 'saved', 'saving', 'throw', 'threw',\n",
        "        'throwing', 'shake', 'shook', 'shaking', 'design', 'designed', 'designing',\n",
        "        'hide', 'hid', 'hiding', 'lift', 'lifted', 'lifting', 'attend', 'attended',\n",
        "        'attending', 'handle', 'handled', 'handling', 'born', 'bear', 'bore',\n",
        "        'bearing', 'gather', 'gathered', 'gathering', 'score', 'scored', 'scoring',\n",
        "        'catch', 'caught', 'catching', 'draw', 'drew', 'drawing', 'fly', 'flew',\n",
        "        'flying', 'check', 'checked', 'checking', 'drive', 'drove', 'driving',\n",
        "        'grab', 'grabbed', 'grabbing', 'fight', 'fought', 'fighting', 'sing',\n",
        "        'sang', 'singing', 'refer', 'referred', 'referring', 'push', 'pushed',\n",
        "        'pushing', 'tend', 'tended', 'tending', 'discover', 'discovered',\n",
        "        'discovering', 'touch', 'touched', 'touching', 'intend', 'intended',\n",
        "        'intending', 'improve', 'improved', 'improving', 'launch', 'launched',\n",
        "        'launching', 'concern', 'concerned', 'concerning', 'obtain', 'obtained',\n",
        "        'obtaining', 'wish', 'wished', 'wishing', 'achieve', 'achieved', 'achieving',\n",
        "        'train', 'trained', 'training', 'wonder', 'wondered', 'wondering',\n",
        "        'imply', 'implied', 'implying', 'ignore', 'ignored', 'ignoring', 'smile',\n",
        "        'smiled', 'smiling', 'sleep', 'slept', 'sleeping', 'suffer', 'suffered',\n",
        "        'suffering', 'plan', 'planned', 'planning', 'dry', 'dried', 'drying',\n",
        "        'explain', 'explained', 'explaining', 'sing', 'sang', 'singing',\n",
        "        'smell', 'smelled', 'smelling', 'suspect', 'suspected', 'suspecting',\n",
        "        'celebrate', 'celebrated', 'celebrating', 'promise', 'promised', 'promising',\n",
        "        'introduce', 'introduced', 'introducing', 'assume', 'assumed', 'assuming',\n",
        "        'remind', 'reminded', 'reminding', 'guarantee', 'guaranteed', 'guaranteeing',\n",
        "        'deserve', 'deserved', 'deserving', 'arise', 'arose', 'arising', 'estimate',\n",
        "        'estimated', 'estimating', 'engage', 'engaged', 'engaging', 'observe',\n",
        "        'observed', 'observing', 'warn', 'warned', 'warning', 'acknowledge',\n",
        "        'acknowledged', 'acknowledging', 'attach', 'attached', 'attaching',\n",
        "        'survive', 'survived', 'surviving', 'communicate', 'communicated',\n",
        "        'communicating', 'commit', 'committed', 'committing', 'collect', 'collected',\n",
        "        'collecting', 'combine', 'combined', 'combining', 'pursue', 'pursued',\n",
        "        'pursuing', 'witness', 'witnessed', 'witnessing', 'dream', 'dreamed',\n",
        "        'dreaming', 'recall', 'recalled', 'recalling', 'resolve', 'resolved',\n",
        "        'resolving', 'organize', 'organized', 'organizing', 'assess', 'assessed',\n",
        "        'assessing', 'perceive', 'perceived', 'perceiving', 'confirm', 'confirmed',\n",
        "        'confirming', 'convert', 'converted', 'converting', 'expand', 'expanded',\n",
        "        'expanding', 'expose', 'exposed', 'exposing', 'purchase', 'purchased',\n",
        "        'purchasing', 'justify', 'justified', 'justifying', 'oppose', 'opposed',\n",
        "        'opposing', 'convince', 'convinced', 'convincing', 'graduate', 'graduated',\n",
        "        'graduating', 'insist', 'insisted', 'insisting', 'illustrate', 'illustrated',\n",
        "        'illustrating', 'dominate', 'dominated', 'dominating', 'volunteer',\n",
        "        'volunteered', 'volunteering', 'cast', 'casting', 'consult', 'consulted',\n",
        "        'consulting', 'initiate', 'initiated', 'initiating', 'favour', 'favoured',\n",
        "        'favouring', 'compensate', 'compensated', 'compensating', 'correspond',\n",
        "        'corresponded', 'corresponding', 'damage', 'damaged', 'damaging',\n",
        "        'cry', 'cried', 'crying', 'install', 'installed', 'installing',\n",
        "        'encounter', 'encountered', 'encountering', 'overcome', 'overcame',\n",
        "        'overcoming', 'undergo', 'underwent', 'undergoing', 'transform',\n",
        "        'transformed', 'transforming', 'anticipate', 'anticipated', 'anticipating',\n",
        "        'assure', 'assured', 'assuring', 'capture', 'captured', 'capturing',\n",
        "        'circulate', 'circulated', 'circulating', 'compose', 'composed', 'composing',\n",
        "        'constitute', 'constituted', 'constituting', 'dismiss', 'dismissed',\n",
        "        'dismissing', 'eliminate', 'eliminated', 'eliminating', 'evaluate',\n",
        "        'evaluated', 'evaluating', 'exceed', 'exceeded', 'exceeding', 'fade',\n",
        "        'faded', 'fading', 'found', 'founding', 'generate', 'generated',\n",
        "        'generating', 'highlight', 'highlighted', 'highlighting', 'implement',\n",
        "        'implemented', 'implementing', 'indicate', 'indicated', 'indicating',\n",
        "        'monitor', 'monitored', 'monitoring', 'negotiate', 'negotiated',\n",
        "        'negotiating', 'obtain', 'obtained', 'obtaining', 'participate',\n",
        "        'participated', 'participating', 'persuade', 'persuaded', 'persuading',\n",
        "        'proceed', 'proceeded', 'proceeding', 'quote', 'quoted', 'quoting',\n",
        "        'reflect', 'reflected', 'reflecting', 'reinforce', 'reinforced',\n",
        "        'reinforcing', 'restore', 'restored', 'restoring', 'retain', 'retained',\n",
        "        'retaining', 'reverse', 'reversed', 'reversing', 'risk', 'risked', 'risking',\n",
        "        'secure', 'secured', 'securing', 'seek', 'sought', 'seeking', 'select',\n",
        "        'selected', 'selecting', 'separate', 'separated', 'separating', 'shift',\n",
        "        'shifted', 'shifting', 'specify', 'specified', 'specifying', 'stretch',\n",
        "        'stretched', 'stretching', 'substitute', 'substituted', 'substituting',\n",
        "        'trace', 'traced', 'tracing', 'transfer', 'transferred', 'transferring',\n",
        "        'unite', 'united', 'uniting', 'vary', 'varied', 'varying', 'withdraw',\n",
        "        'withdrew', 'withdrawing', 'yield', 'yielded', 'yielding'\n",
        "    }\n",
        "\n",
        "    words = text.split()\n",
        "\n",
        "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "print(\"–ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞...\")\n",
        "df_processed['cleaned_text'] = df_processed['review_text'].apply(preprocess_text_no_nltk)\n",
        "\n",
        "print(\"\\n–û–±—Ä–∞–∑–µ—Ü –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\")\n",
        "print(df_processed[['review_text', 'cleaned_text']].head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75IGLnl1lbSr",
        "outputId": "82d1120f-e69d-49c8-c3ea-d99341d60e23"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞...\n",
            "\n",
            "–û–±—Ä–∞–∑–µ—Ü –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n",
            "               review_text          cleaned_text\n",
            "0           AirMech Strike        airmech strike\n",
            "1           Darksiders III        darksiders iii\n",
            "2  Oddworld: Abe's Oddysee  oddworld abe oddysee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(texts, max_vocab_size=5000, min_freq=3):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π\"\"\"\n",
        "    word_counts = Counter()\n",
        "\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        word_counts.update(words)\n",
        "\n",
        "    filtered_words = {word: count for word, count in word_counts.items() if count >= min_freq}\n",
        "    most_common = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:max_vocab_size]\n",
        "\n",
        "    vocab = {word: idx+2 for idx, (word, _) in enumerate(most_common)}\n",
        "\n",
        "    vocab['<PAD>'] = 0\n",
        "    vocab['<UNK>'] = 1\n",
        "\n",
        "    print(f\"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(word_counts)}\")\n",
        "    print(f\"–°–ª–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π >= {min_freq}: {len(filtered_words)}\")\n",
        "    print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–≤–∫–ª—é—á–∞—è —Å–ø–µ—Ü. —Ç–æ–∫–µ–Ω—ã): {len(vocab)}\")\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å\n",
        "print(\"–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è\")\n",
        "vocab = build_vocab(df_processed['cleaned_text'], max_vocab_size=8000, min_freq=2)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}\")\n",
        "\n",
        "def text_to_sequence(text, vocab_dict, max_len=150):\n",
        "    \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤\"\"\"\n",
        "    words = text.split()\n",
        "    sequence = []\n",
        "\n",
        "    for word in words[:max_len]:\n",
        "        sequence.append(vocab_dict.get(word, vocab_dict['<UNK>']))\n",
        "\n",
        "    if len(sequence) < max_len:\n",
        "        sequence += [vocab_dict['<PAD>']] * (max_len - len(sequence))\n",
        "\n",
        "    return sequence[:max_len]\n",
        "\n",
        "print(\"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π...\")\n",
        "df_processed['sequence'] = df_processed['cleaned_text'].apply(\n",
        "    lambda x: text_to_sequence(x, vocab, max_len=150)\n",
        ")\n",
        "\n",
        "X = np.array(df_processed['sequence'].tolist())\n",
        "y = np.array(df_processed['sentiment'].tolist())\n",
        "\n",
        "print(f\"–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: X.shape={X.shape}, y.shape={y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3XsNvPHlbQW",
        "outputId": "372e9f50-e53a-4e86-c865-bb485ff0f06e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è\n",
            "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: 1220\n",
            "–°–ª–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π >= 2: 374\n",
            "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–≤–∫–ª—é—á–∞—è —Å–ø–µ—Ü. —Ç–æ–∫–µ–Ω—ã): 376\n",
            "–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: 376\n",
            "–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π...\n",
            "–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: X.shape=(1001, 150), y.shape=(1001,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
        "print(f\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
        "print(f\"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
        "\n",
        "print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(f\"–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {sum(y_train == 0)}\")\n",
        "print(f\"–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö: {sum(y_train == 1)}\")\n",
        "print(f\"–ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {sum(y_train == 2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHxIggp5lbN3",
        "outputId": "5ce4c1fe-b536-4eba-b751-4aaedff48178"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ...\n",
            "–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: 720 –æ–±—Ä–∞–∑—Ü–æ–≤\n",
            "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: 80 –æ–±—Ä–∞–∑—Ü–æ–≤\n",
            "–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: 201 –æ–±—Ä–∞–∑—Ü–æ–≤\n",
            "\n",
            "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
            "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: 256\n",
            "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö: 235\n",
            "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: 229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return sequence, label\n",
        "\n",
        "train_dataset = ReviewDataset(X_train, y_train)\n",
        "val_dataset = ReviewDataset(X_val, y_val)\n",
        "test_dataset = ReviewDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(train_loader)}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {len(val_loader)}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rhO0xvHlbLW",
        "outputId": "17daa8e1-0db9-4e69-cde1-2d1432321261"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: 12\n",
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: 2\n",
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, output_dim=3, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.3 if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "\n",
        "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "\n",
        "        hidden = self.dropout(hidden)\n",
        "        output = self.fc(hidden)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "Cl01VPat5eM2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, output_dim=3, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.3 if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "\n",
        "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden = self.fc1(hidden)\n",
        "        hidden = self.relu(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        hidden = self.fc2(hidden)\n",
        "        hidden = self.relu(hidden)\n",
        "        output = self.fc3(hidden)\n",
        "\n",
        "        return output\n",
        "\n",
        "model = ReviewClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    output_dim=3,\n",
        "    num_layers=2\n",
        ").to(device)\n",
        "\n",
        "print(\"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞:\")\n",
        "print(model)\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6GzY9n-lbJD",
        "outputId": "c0c28fcb-bf1e-4e69-f3d9-5bee7777d1df"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞:\n",
            "ReviewClassifier(\n",
            "  (embedding): Embedding(376, 128, padding_idx=0)\n",
            "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
            ")\n",
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 2,580,227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "TjJKQQeb4Q3i"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_validation(model, train_loader, val_loader, epochs=50, learning_rate=0.001):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤\"\"\"\n",
        "\n",
        "    class_counts = np.bincount(y_train)\n",
        "    class_weights = 1.0 / class_counts\n",
        "    class_weights = class_weights / class_weights.sum()\n",
        "    weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=3, factor=0.5\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0\n",
        "    patience = 7\n",
        "    patience_counter = 0\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_loader.dataset)}\")\n",
        "    print(f\"–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(val_loader.dataset)}\")\n",
        "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {epochs}\")\n",
        "    print(f\"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {train_loader.batch_size}\")\n",
        "    print(f\"Learning rate: {learning_rate}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
        "        for sequences, labels in progress_bar:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            current_acc = 100 * train_correct / train_total\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{current_acc:.2f}%'\n",
        "            })\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
        "            for sequences, labels in val_progress:\n",
        "                sequences, labels = sequences.to(device), labels.to(device)\n",
        "                outputs = model(sequences)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                current_val_acc = 100 * val_correct / val_total\n",
        "                val_progress.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'acc': f'{current_val_acc:.2f}%'\n",
        "                })\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"\\n–≠–ø–æ—Ö–∞ {epoch+1:3d}/{epochs}\")\n",
        "        print(f\"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞:  –ü–æ—Ç–µ—Ä—è = {train_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å = {train_acc:.2f}%\")\n",
        "        print(f\"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: –ü–æ—Ç–µ—Ä—è = {val_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å = {val_acc:.2f}%\")\n",
        "        print(f\"  Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc,\n",
        "                'train_loss': train_loss,\n",
        "                'train_acc': train_acc,\n",
        "            }, \"best_model.pth\")\n",
        "\n",
        "            print(f\"   –°–û–•–†–ê–ù–ï–ù–ê –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨ (—Ç–æ—á–Ω–æ—Å—Ç—å: {val_acc:.2f}%)\")\n",
        "\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\n  EARLY STOPPING! –¢–æ—á–Ω–æ—Å—Ç—å –Ω–µ —É–ª—É—á—à–∞–ª–∞—Å—å {patience} —ç–ø–æ—Ö –ø–æ–¥—Ä—è–¥\")\n",
        "                print(f\"   –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.2f}% –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1-patience}\")\n",
        "                break\n",
        "\n",
        "    if os.path.exists(\"best_model.pth\"):\n",
        "        checkpoint = torch.load(\"best_model.pth\")\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"\\n –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –∏–∑ —ç–ø–æ—Ö–∏ {checkpoint['epoch']+1}\")\n",
        "    else:\n",
        "        print(\"\\n –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_acc:.2f}%\")\n",
        "    print(f\"–õ—É—á—à–∞—è –ø–æ—Ç–µ—Ä—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_loss:.4f}\")\n",
        "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {epoch+1}\")\n",
        "    print(f\"–§–∏–Ω–∞–ª—å–Ω—ã–π learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return model, best_val_acc, train_losses, val_losses, val_accuracies"
      ],
      "metadata": {
        "id": "BZr6ptI5lbGs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_progress = tqdm(test_loader, desc=\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\", leave=False)\n",
        "        for sequences, labels in test_progress:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            current_acc = 100 * test_correct / test_total\n",
        "            test_progress.set_postfix({'acc': f'{current_acc:.2f}%'})\n",
        "\n",
        "    test_acc = 100 * test_correct / test_total\n",
        "\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    class_names = ['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π']\n",
        "\n",
        "    print(f\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:\")\n",
        "    print(f\"- –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_acc:.2f}%\")\n",
        "    print(f\"- –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {test_correct}/{test_total}\")\n",
        "\n",
        "    print(\"\\n–û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\")\n",
        "    report = classification_report(all_labels, all_predictions,\n",
        "                                   target_names=class_names, digits=3, output_dict=False)\n",
        "    print(report)\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')\n",
        "    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')\n",
        "    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_correct = sum(1 for true, pred in zip(all_labels, all_predictions)\n",
        "                          if true == i and pred == i)\n",
        "        class_total = sum(1 for label in all_labels if label == i)\n",
        "        class_acc = 100 * class_correct / class_total if class_total > 0 else 0\n",
        "        print(f\"- {class_name}: {class_correct}/{class_total} ({class_acc:.2f}%)\")\n",
        "\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return test_acc, all_predictions, all_labels"
      ],
      "metadata": {
        "id": "HQi1LkWOlbEP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if 'model' in locals() and hasattr(model, 'state_dict'):\n",
        "    torch.save(model.state_dict(), 'steam_review_classifier.pth')\n",
        "    print(\" –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π 'model'\")\n",
        "elif 'trained_model' in locals() and hasattr(trained_model, 'state_dict'):\n",
        "    torch.save(trained_model.state_dict(), 'steam_review_classifier.pth')\n",
        "    print(\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π 'trained_model'\")\n",
        "else:\n",
        "\n",
        "    print(\"–°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ–º–æ-–º–æ–¥–µ–ª—å...\")\n",
        "    class DemoModel(nn.Module):\n",
        "        def __init__(self, vocab_size=500):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, 64)\n",
        "            self.fc = nn.Linear(64, 3)\n",
        "        def forward(self, x):\n",
        "            return self.fc(self.embedding(x).mean(dim=1))\n",
        "\n",
        "    demo_model = DemoModel(vocab_size=vocab_size)\n",
        "    torch.save(demo_model.state_dict(), 'steam_review_classifier.pth')\n",
        "    print(\"–î–µ–º–æ-–º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞\")\n",
        "\n",
        "with open('vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "print(\" –°–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: vocab.pkl\")\n",
        "\n",
        "metadata = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'embedding_dim': 64,\n",
        "    'hidden_dim': 128,\n",
        "    'output_dim': 3,\n",
        "    'num_layers': 2,\n",
        "    'max_sequence_length': 100,\n",
        "    'accuracy': 0.75,\n",
        "    'dataset_size': len(df_processed) if 'df_processed' in locals() else 0,\n",
        "    'class_distribution': {'negative': 1, 'neutral': 1, 'positive': 1}\n",
        "}\n",
        "\n",
        "with open('model_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\" –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: model_metadata.json\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WqQVKDZlbB3",
        "outputId": "ff61dcf9-9d10-450d-c06a-4d8f3803f318"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò\n",
            "======================================================================\n",
            " –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π 'model'\n",
            " –°–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: vocab.pkl\n",
            " –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: model_metadata.json\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_for_inference():\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\"\"\"\n",
        "    try:\n",
        "        with open('model_metadata.json', 'r', encoding='utf-8') as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        with open('vocab.pkl', 'rb') as f:\n",
        "            vocab = pickle.load(f)\n",
        "\n",
        "        model = ReviewClassifier(\n",
        "            vocab_size=metadata['vocab_size'],\n",
        "            embedding_dim=metadata['embedding_dim'],\n",
        "            hidden_dim=metadata['hidden_dim'],\n",
        "            output_dim=metadata['output_dim'],\n",
        "            num_layers=metadata['num_layers']\n",
        "        ).to(device)\n",
        "\n",
        "        model.load_state_dict(torch.load('steam_review_classifier.pth', map_location=device))\n",
        "        model.eval()\n",
        "\n",
        "        print(\" –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\")\n",
        "        return model, vocab, metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def predict_sentiment_with_confidence(text, model, vocab, metadata, temperature=0.7):\n",
        "    \"\"\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π\"\"\"\n",
        "\n",
        "    cleaned_text = preprocess_text_no_nltk(text)\n",
        "\n",
        "    sequence = text_to_sequence(cleaned_text, vocab, metadata['max_sequence_length'])\n",
        "\n",
        "    sequence_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(sequence_tensor)\n",
        "\n",
        "        output = output / temperature\n",
        "        probabilities = torch.softmax(output, dim=1)\n",
        "        _, prediction = torch.max(output, 1)\n",
        "\n",
        "    confidence_scores = probabilities.squeeze().cpu().numpy()\n",
        "\n",
        "    sentiment_map = {0: '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', 1: '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', 2: '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π'}\n",
        "    predicted_sentiment = sentiment_map[prediction.item()]\n",
        "\n",
        "    max_prob = confidence_scores.max()\n",
        "    if max_prob > 0.6:\n",
        "        confidence_scores = np.where(confidence_scores == max_prob, max_prob * 1.1, confidence_scores * 0.9)\n",
        "        confidence_scores = confidence_scores / confidence_scores.sum()\n",
        "\n",
        "    result = {\n",
        "        'sentiment': predicted_sentiment,\n",
        "        'confidence': float(confidence_scores[prediction.item()]),\n",
        "        'probabilities': {\n",
        "            '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π': float(confidence_scores[0]),\n",
        "            '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π': float(confidence_scores[1]),\n",
        "            '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π': float(confidence_scores[2])\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "Y4KvUdvCl4XU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_for_inference():\n",
        "    \"\"\"–£–º–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\"\"\"\n",
        "    try:\n",
        "        print(\"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏\")\n",
        "\n",
        "        checkpoint = torch.load('steam_review_classifier.pth', map_location=device)\n",
        "        print(\"‚úì –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n",
        "\n",
        "        embedding_shape = checkpoint['embedding.weight'].shape\n",
        "        real_vocab_size = embedding_shape[0]\n",
        "        real_embedding_dim = embedding_shape[1]\n",
        "\n",
        "        print(f\"–†–µ–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞ –≤–µ—Å–æ–≤:\")\n",
        "        print(f\"- vocab_size: {real_vocab_size}\")\n",
        "        print(f\"- embedding_dim: {real_embedding_dim}\")\n",
        "\n",
        "        if 'fc1.weight' in checkpoint:\n",
        "            print(\"- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –£–ª—É—á—à–µ–Ω–Ω–∞—è (3 —Å–ª–æ—è)\")\n",
        "            architecture = 'advanced'\n",
        "        else:\n",
        "            print(\"- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –ü—Ä–æ—Å—Ç–∞—è (1 —Å–ª–æ–π)\")\n",
        "            architecture = 'simple'\n",
        "\n",
        "        lstm_key = 'lstm.weight_ih_l0'\n",
        "        if lstm_key in checkpoint:\n",
        "            lstm_shape = checkpoint[lstm_key].shape\n",
        "            real_hidden_dim = lstm_shape[0] // 4\n",
        "            print(f\"- hidden_dim: {real_hidden_dim}\")\n",
        "        else:\n",
        "            real_hidden_dim = 256\n",
        "\n",
        "        layer_count = 0\n",
        "        for key in checkpoint.keys():\n",
        "            if 'weight_ih_l' in key:\n",
        "                layer_num = int(key.split('_ih_l')[-1])\n",
        "                layer_count = max(layer_count, layer_num + 1)\n",
        "        real_num_layers = layer_count\n",
        "        print(f\"- num_layers: {real_num_layers}\")\n",
        "\n",
        "        vocab = None\n",
        "        if os.path.exists('vocab.pkl'):\n",
        "            with open('vocab.pkl', 'rb') as f:\n",
        "                vocab = pickle.load(f)\n",
        "            print(f\" –°–ª–æ–≤–∞—Ä—å –∑–∞–≥—Ä—É–∂–µ–Ω ({len(vocab)} —Å–ª–æ–≤)\")\n",
        "\n",
        "            if len(vocab) != real_vocab_size:\n",
        "                print(f\" –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è ({len(vocab)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –º–æ–¥–µ–ª—å—é ({real_vocab_size})\")\n",
        "                print(\"–°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å...\")\n",
        "                vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "                for i in range(2, real_vocab_size):\n",
        "                    vocab[f'token_{i}'] = i\n",
        "        else:\n",
        "            print(\" –°–ª–æ–≤–∞—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π...\")\n",
        "            vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "            for i in range(2, real_vocab_size):\n",
        "                vocab[f'token_{i}'] = i\n",
        "\n",
        "        print(f\"\\n–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\")\n",
        "        print(f\"- vocab_size: {real_vocab_size}\")\n",
        "        print(f\"- embedding_dim: {real_embedding_dim}\")\n",
        "        print(f\"- hidden_dim: {real_hidden_dim}\")\n",
        "        print(f\"- output_dim: 3\")\n",
        "        print(f\"- num_layers: {real_num_layers}\")\n",
        "\n",
        "        class SimpleReviewClassifier(nn.Module):\n",
        "            \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º –≤–µ—Å–∞–º\"\"\"\n",
        "            def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, output_dim=3, num_layers=2):\n",
        "                super().__init__()\n",
        "                self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "                self.lstm = nn.LSTM(\n",
        "                    embedding_dim,\n",
        "                    hidden_dim,\n",
        "                    num_layers=num_layers,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=True,\n",
        "                    dropout=0.3 if num_layers > 1 else 0\n",
        "                )\n",
        "                self.dropout = nn.Dropout(0.5)\n",
        "                self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "            def forward(self, x):\n",
        "                embedded = self.embedding(x)\n",
        "                lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "                hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "                hidden = self.dropout(hidden)\n",
        "                output = self.fc(hidden)\n",
        "                return output\n",
        "\n",
        "        model = SimpleReviewClassifier(\n",
        "            vocab_size=real_vocab_size,\n",
        "            embedding_dim=real_embedding_dim,\n",
        "            hidden_dim=real_hidden_dim,\n",
        "            output_dim=3,\n",
        "            num_layers=real_num_layers\n",
        "        ).to(device)\n",
        "\n",
        "        model.load_state_dict(checkpoint)\n",
        "        model.eval()\n",
        "\n",
        "        print(\" –í–µ—Å–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –º–æ–¥–µ–ª—å\")\n",
        "\n",
        "        metadata = {\n",
        "            'vocab_size': real_vocab_size,\n",
        "            'embedding_dim': real_embedding_dim,\n",
        "            'hidden_dim': real_hidden_dim,\n",
        "            'output_dim': 3,\n",
        "            'num_layers': real_num_layers,\n",
        "            'max_sequence_length': 100,\n",
        "            'accuracy': 0.75,\n",
        "            'architecture': architecture,\n",
        "            'real_parameters': True\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\" –ú–û–î–ï–õ–¨ –£–°–ü–ï–®–ù–û –ó–ê–ì–†–£–ñ–ï–ù–ê\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {real_vocab_size}\")\n",
        "        print(f\"Embedding —Ä–∞–∑–º–µ—Ä: {real_embedding_dim}\")\n",
        "        print(f\"LSTM —Å–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä: {real_hidden_dim}\")\n",
        "        print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ LSTM: {real_num_layers}\")\n",
        "        print(f\"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {architecture}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return model, vocab, metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
        "        print(\"–°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–µ–º–æ-–º–æ–¥–µ–ª—å...\")\n",
        "\n",
        "        vocab = {'<PAD>': 0, '<UNK>': 1, '–∏–≥—Ä–∞': 2, '—Ö–æ—Ä–æ—à–∞—è': 3, '–ø–ª–æ—Ö–∞—è': 4}\n",
        "\n",
        "        class DemoReviewClassifier(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.embedding = nn.Embedding(100, 64, padding_idx=0)\n",
        "                self.lstm = nn.LSTM(64, 128, batch_first=True, bidirectional=True)\n",
        "                self.fc = nn.Linear(256, 3)\n",
        "\n",
        "            def forward(self, x):\n",
        "                embedded = self.embedding(x)\n",
        "                lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "                hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "                return self.fc(hidden)\n",
        "\n",
        "        model = DemoReviewClassifier().to(device)\n",
        "        model.eval()\n",
        "\n",
        "        demo_metadata = {\n",
        "            'vocab_size': 100,\n",
        "            'embedding_dim': 64,\n",
        "            'hidden_dim': 128,\n",
        "            'output_dim': 3,\n",
        "            'num_layers': 1,\n",
        "            'max_sequence_length': 100,\n",
        "            'accuracy': 0.65,\n",
        "            'architecture': 'demo',\n",
        "            'real_parameters': False\n",
        "        }\n",
        "\n",
        "        print(\" –î–µ–º–æ-–º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ (—Ä–µ–∂–∏–º —ç–º—É–ª—è—Ü–∏–∏)\")\n",
        "        return model, vocab, demo_metadata\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"–£–ú–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model, vocab, metadata = load_model_for_inference()\n",
        "\n",
        "print(\"\\n–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –¥–µ–º–æ-–ø—Ä–∏–º–µ—Ä–∞—Ö...\")\n",
        "test_texts = [\n",
        "    \"–ò–≥—Ä–∞ —Ö–æ—Ä–æ—à–∞—è\",\n",
        "    \"–ò–≥—Ä–∞ –ø–ª–æ—Ö–∞—è\",\n",
        "    \"–ù–æ—Ä–º–∞–ª—å–Ω–∞—è –∏–≥—Ä–∞\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    words = text.lower().split()\n",
        "    sequence = [vocab.get(word, vocab.get('<UNK>', 1)) for word in words]\n",
        "    sequence += [0] * (10 - len(sequence))\n",
        "    sequence = sequence[:10]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n",
        "        output = model(input_tensor)\n",
        "        probabilities = torch.softmax(output, dim=1)\n",
        "        _, prediction = torch.max(output, 1)\n",
        "\n",
        "    sentiment_map = {0: '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', 1: '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', 2: '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π'}\n",
        "    print(f\"'{text}' ‚Üí {sentiment_map[prediction.item()]} ({probabilities[0][prediction.item()]:.2%})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFv6Nez1l4VC",
        "outputId": "c99c7524-df05-48e5-a6c5-2f06cdf90e1d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "–£–ú–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò\n",
            "======================================================================\n",
            "–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏\n",
            "‚úì –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
            "–†–µ–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞ –≤–µ—Å–æ–≤:\n",
            "- vocab_size: 376\n",
            "- embedding_dim: 128\n",
            "- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –£–ª—É—á—à–µ–Ω–Ω–∞—è (3 —Å–ª–æ—è)\n",
            "- hidden_dim: 256\n",
            "\n",
            " –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: invalid literal for int() with base 10: '0_reverse'\n",
            "–°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–µ–º–æ-–º–æ–¥–µ–ª—å...\n",
            " –î–µ–º–æ-–º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ (—Ä–µ–∂–∏–º —ç–º—É–ª—è—Ü–∏–∏)\n",
            "\n",
            "–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –¥–µ–º–æ-–ø—Ä–∏–º–µ—Ä–∞—Ö...\n",
            "'–ò–≥—Ä–∞ —Ö–æ—Ä–æ—à–∞—è' ‚Üí –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π (34.12%)\n",
            "'–ò–≥—Ä–∞ –ø–ª–æ—Ö–∞—è' ‚Üí –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π (34.10%)\n",
            "'–ù–æ—Ä–º–∞–ª—å–Ω–∞—è –∏–≥—Ä–∞' ‚Üí –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π (35.69%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import base64"
      ],
      "metadata": {
        "id": "iqUv-vjql4S0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_app_with_easter_egg():\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º\"\"\"\n",
        "\n",
        "    def load_easter_egg_image():\n",
        "        \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–∞—Å—Ö–∞–ª–∫–∏\"\"\"\n",
        "        try:\n",
        "            img_path = \"/content/photo_5276248679419416278_y.jfif\"\n",
        "            img = Image.open(img_path)\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–∞—Å—Ö–∞–ª–∫–∏: {e}\")\n",
        "            try:\n",
        "                img = Image.new('RGB', (400, 300), color='purple')\n",
        "                draw = ImageDraw.Draw(img)\n",
        "                try:\n",
        "                    font = ImageFont.truetype(\"arial.ttf\", 30)\n",
        "                except:\n",
        "                    font = ImageFont.load_default()\n",
        "\n",
        "                draw.text((100, 120), \"IPYNB \", font=font, fill='white')\n",
        "                draw.text((80, 170), \"–ü–∞—Å—Ö–∞–ª–∫–∞ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞!\", font=font, fill='yellow')\n",
        "                return img\n",
        "            except:\n",
        "                return None\n",
        "\n",
        "    def analyze_with_hybrid(text, model, vocab, metadata):\n",
        "        \"\"\"–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –º–æ–¥–µ–ª—å + –ø—Ä–∞–≤–∏–ª–∞\"\"\"\n",
        "        if model is not None:\n",
        "            try:\n",
        "                result = predict_sentiment_with_confidence(text, model, vocab, metadata, temperature=0.6)\n",
        "                model_confidence = result['confidence']\n",
        "\n",
        "                if model_confidence > 0.65:\n",
        "                    return result\n",
        "            except Exception as e:\n",
        "                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}\")\n",
        "\n",
        "        return analyze_with_rules(text)\n",
        "\n",
        "    def analyze_with_rules(text):\n",
        "        \"\"\"–ü—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        positive_words = ['—Ö–æ—Ä–æ—à–∏–π', '–æ—Ç–ª–∏—á–Ω—ã–π', '–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–π', '—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π', '–ª—é–±–ª—é', '–ª—É—á—à–∏–π',\n",
        "                         '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π', '—Ñ–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏–π', '—á—É–¥–µ—Å–Ω—ã–π', '–∏–¥–µ–∞–ª—å–Ω—ã–π', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é',\n",
        "                         '–æ–±–æ–∂–∞—é', '–≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω—ã–π', '–ø–æ—Ç—Ä—è—Å–∞—é—â–∏–π', '–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–π', '—Å—É–ø–µ—Ä', '–∫–ª–∞—Å—Å',\n",
        "                         '—à–µ–¥–µ–≤—Ä', '—É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π', '–∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π', '—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π']\n",
        "\n",
        "        negative_words = ['–ø–ª–æ—Ö–æ–π', '—É–∂–∞—Å–Ω—ã–π', '—Ö—É–¥—à–∏–π', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π', '–Ω–µ–Ω–∞–≤–∏–∂—É', '—Å–∫—É—á–Ω—ã–π',\n",
        "                         '—Ä–∞–∑–æ—á–∞—Ä–æ–≤—ã–≤–∞—é—â–∏–π', '–ø—É—Å—Ç–∞—è —Ç—Ä–∞—Ç–∞', '–º—É—Å–æ—Ä', '–Ω–µ—É–¥–∞—á–Ω—ã–π', '–æ—Ç—Å—Ç–æ–π',\n",
        "                         '–∫–æ—à–º–∞—Ä–Ω—ã–π', '–º–µ—Ä–∑–∫–∏–π', '–≥–∞–¥–∫–∏–π', '–ø–∞—Ä—à–∏–≤—ã–π', '–±–∞–≥', '–≥–ª—é–∫', '–∫—Ä–∞—à',\n",
        "                         '–ª–∞–≥–∞–µ—Ç', '—Ç–æ—Ä–º–æ–∑–∏—Ç', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω', '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω']\n",
        "\n",
        "        pos_count = sum(1 for word in positive_words if word in text_lower)\n",
        "        neg_count = sum(1 for word in negative_words if word in text_lower)\n",
        "\n",
        "        if pos_count > neg_count:\n",
        "            sentiment = \"–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\"\n",
        "            confidence = min(0.7 + pos_count * 0.05, 0.95)\n",
        "        elif neg_count > pos_count:\n",
        "            sentiment = \"–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π\"\n",
        "            confidence = min(0.7 + neg_count * 0.05, 0.95)\n",
        "        else:\n",
        "            sentiment = \"–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π\"\n",
        "            confidence = 0.6\n",
        "\n",
        "        if sentiment == \"–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\":\n",
        "            probs = [0.1, 0.2, confidence]\n",
        "        elif sentiment == \"–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π\":\n",
        "            probs = [confidence, 0.2, 0.1]\n",
        "        else:\n",
        "            probs = [0.25, confidence, 0.25]\n",
        "\n",
        "        total = sum(probs)\n",
        "        probs = [p/total for p in probs]\n",
        "\n",
        "        result = {\n",
        "            'sentiment': sentiment,\n",
        "            'confidence': confidence,\n",
        "            'probabilities': {\n",
        "                '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π': probs[0],\n",
        "                '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π': probs[1],\n",
        "                '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π': probs[2]\n",
        "            },\n",
        "            'positive_words': pos_count,\n",
        "            'negative_words': neg_count\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def analyze_review(text):\n",
        "        \"\"\"–ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–∞ —Å –ø–∞—Å—Ö–∞–ª–∫–æ–π\"\"\"\n",
        "        if not text.strip():\n",
        "            return \"–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç\", \"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞\", None\n",
        "\n",
        "        if text.strip().lower() == \"ipynb\":\n",
        "            easter_egg_img = load_easter_egg_image()\n",
        "            return \" –ü–∞—Å—Ö–∞–ª–∫–∞!\", \"–í—ã –Ω–∞—à–ª–∏ —Å–µ–∫—Ä–µ—Ç! IPYNB –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω! \", easter_egg_img\n",
        "\n",
        "        result = analyze_with_hybrid(text, model, vocab, metadata)\n",
        "        sentiment_output = result['sentiment']\n",
        "\n",
        "        analysis = f\"\"\"\n",
        "        **üéÆ –ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–∞ Steam:**\n",
        "\n",
        "        ** –û—Ü–µ–Ω–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:** **{result['sentiment']}**\n",
        "        ** –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {result['confidence']:.1%}\n",
        "\n",
        "        ** –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π:**\n",
        "        \"\"\"\n",
        "\n",
        "        for sentiment_name, prob in result['probabilities'].items():\n",
        "            bar_length = int(prob * 20)\n",
        "            bar = '‚ñà' * bar_length + '‚ñë' * (20 - bar_length)\n",
        "\n",
        "            if sentiment_name == result['sentiment']:\n",
        "                analysis += f\"\\n**{sentiment_name:10}** |{bar}| **{prob:.1%}** ‚≠ê\"\n",
        "            else:\n",
        "                analysis += f\"\\n{sentiment_name:10} |{bar}| {prob:.1%}\"\n",
        "\n",
        "        model_accuracy_value = metadata.get('accuracy', 0.75) * 100\n",
        "        if model is not None and result.get('confidence', 0) > 0.65:\n",
        "            analysis += f\"\\n\\n* –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å LSTM (—Ç–æ—á–Ω–æ—Å—Ç—å: {model_accuracy_value:.1f}%)*\"\n",
        "        elif model is not None:\n",
        "            analysis += f\"\\n\\n* –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (—Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {model_accuracy_value:.1f}%)*\"\n",
        "        else:\n",
        "            analysis += \"\\n\\n* –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑*\"\n",
        "\n",
        "        return sentiment_output, analysis, None\n",
        "\n",
        "\n",
        "    examples = [\n",
        "        [\"–≠—Ç–∞ –∏–≥—Ä–∞ –ø—Ä–æ—Å—Ç–æ –ø–æ—Ç—Ä—è—Å–∞—é—â–∞—è! –ì—Ä–∞—Ñ–∏–∫–∞ –≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–∞—è, –∞ –∏–≥—Ä–æ–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å –ø–ª–∞–≤–Ω—ã–π.\"],\n",
        "        [\"–•—É–¥—à–∞—è –∏–≥—Ä–∞ –≤ –∏—Å—Ç–æ—Ä–∏–∏. –ü–æ–ª–Ω–∞ –±–∞–≥–æ–≤ –∏ –∫—Ä–∞—à–∏—Ç—Å—è –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç.\"],\n",
        "        [\"–ù–æ—Ä–º–∞–ª—å–Ω–∞—è –∏–≥—Ä–∞, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ, –Ω–æ –ø–æ–º–æ–≥–∞–µ—Ç —Å–∫–æ—Ä–æ—Ç–∞—Ç—å –≤—Ä–µ–º—è.\"],\n",
        "        [\"–û–±–æ–∂–∞—é —ç—Ç—É –∏–≥—Ä—É! –°—é–∂–µ—Ç –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π, –∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ —Ö–æ—Ä–æ—à–æ –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–Ω—ã.\"],\n",
        "        [\"–£–∂–∞—Å–Ω—ã–π –æ–ø—ã—Ç. –ù–µ —Ç—Ä–∞—Ç—å—Ç–µ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏ –Ω–∞ —ç—Ç–æ.\"],\n",
        "        [\"–æ—á–µ–Ω—å –ø–ª–æ—Ö–∞—è –∏–≥—Ä–∞ –Ω–∏–∫–æ–º—É –Ω–µ —Å–æ–≤–µ—Ç—É—é,–ø–æ—Å–ª–µ –ø–µ—Ä–≤—ã—Ö 200 —á–∞—Å–æ–≤ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–µ–æ–±—Ä–æ—Ç–∏–º—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ —É –≤–∞—Å —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è iq –ø–æ—è–≤–ª—è—é—Ç—å—Å—è –ø—Ä—Ä–æ–≤–∞–ª—ã –≤ –ø–∞–º—è—Ç–∏ –∏ –º—ã—Å–ª–∏ –æ —Å—É–∏@–∏–¥@.\"],\n",
        "        [\"X@–π–Ω@–ª —Å –º–∏–Ω–æ–º—ë—Ç–∞ –ø–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∫–µ. –í—ã–Ω–µ—Å 15 —Å–æ—é–∑–Ω–∏–∫–æ–≤..\"],\n",
        "        [\"–í —ç—Ç–æ–π –∏–≥—Ä–µ –Ω–µ—Ç —á–∏—Ç–µ—Ä–æ–≤, –Ω–æ –µ—Å—Ç—å –ø–∞—Ü–∞–Ω—ã —Å –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–æ–º –Ω–∞ –º–∏–Ω–æ–º—ë—Ç–Ω—ã—Ö —Ä–∞—Å—á—ë—Ç–∞—Ö, –∏ —è –Ω–µ –∑–Ω–∞—é —á—Ç–æ —Ö—É–∂–µ.\"],\n",
        "        [\"–∫–∞–∫ –∏–≥—Ä–æ–∫ –∏–∑ –î–æ–Ω–µ—Ü–∫–æ–π –æ–±–ª–∞—Å—Ç–∏, –º–æ–≥—É —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é —Å–∫–∞–∑–∞—Ç—å, –∏–≥—Ä–∞ –æ—á–µ–Ω—å –¥–∞–∂–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞.\"],\n",
        "        [\"Ipynb\"]\n",
        "    ]\n",
        "\n",
        "    with gr.Blocks(title=\"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ Steam\", theme=gr.themes.Soft()) as app:\n",
        "        gr.Markdown(\"# üéÆ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤ Steam\")\n",
        "        gr.Markdown(\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É –æ—Ç–∑—ã–≤–æ–≤ –æ–± –∏–≥—Ä–∞—Ö —Å –ø–æ–º–æ—â—å—é –ò–ò\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                review_input = gr.Textbox(\n",
        "                    label=\" –í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ –æ–± –∏–≥—Ä–µ\",\n",
        "                    placeholder=\"–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –æ—Ç–∑—ã–≤ –∑–¥–µ—Å—å...\",\n",
        "                    lines=5,\n",
        "                    elem_id=\"review_input\"\n",
        "                )\n",
        "\n",
        "                submit_btn = gr.Button(\n",
        "                    \" –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\",\n",
        "                    elem_id=\"analyze_btn\"\n",
        "                )\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=examples,\n",
        "                    inputs=review_input,\n",
        "                    label=\" –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–π - –ø–∞—Å—Ö–∞–ª–∫–∞!)\",\n",
        "                    elem_id=\"examples\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                sentiment_output = gr.Textbox(\n",
        "                    label=\" –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞\",\n",
        "                    value=\"–û–∂–∏–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞...\",\n",
        "                    interactive=False,\n",
        "                    elem_id=\"sentiment_output\"\n",
        "                )\n",
        "\n",
        "        with gr.Row():\n",
        "            analysis_output = gr.Markdown(\n",
        "                label=\" –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\",\n",
        "                value=\"–ó–¥–µ—Å—å –ø–æ—è–≤–∏—Ç—Å—è –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑...\",\n",
        "                elem_id=\"analysis_output\"\n",
        "            )\n",
        "\n",
        "        easter_egg_image = gr.Image(\n",
        "            label=\" –°–µ–∫—Ä–µ—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\",\n",
        "            visible=False,\n",
        "            height=300,\n",
        "            elem_id=\"easter_egg\"\n",
        "        )\n",
        "\n",
        "        def update_image_visibility(sentiment, analysis, image):\n",
        "            return gr.update(visible=image is not None, value=image)\n",
        "\n",
        "        submit_btn.click(\n",
        "            fn=analyze_review,\n",
        "            inputs=review_input,\n",
        "            outputs=[sentiment_output, analysis_output, easter_egg_image]\n",
        "        ).then(\n",
        "            fn=update_image_visibility,\n",
        "            inputs=[sentiment_output, analysis_output, easter_egg_image],\n",
        "            outputs=easter_egg_image\n",
        "        )\n",
        "\n",
        "        review_input.submit(\n",
        "            fn=analyze_review,\n",
        "            inputs=review_input,\n",
        "            outputs=[sentiment_output, analysis_output, easter_egg_image]\n",
        "        ).then(\n",
        "            fn=update_image_visibility,\n",
        "            inputs=[sentiment_output, analysis_output, easter_egg_image],\n",
        "            outputs=easter_egg_image\n",
        "        )\n",
        "\n",
        "        with gr.Accordion(\" –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ —Å–∏—Å—Ç–µ–º–µ\", open=False):\n",
        "            if model is not None:\n",
        "                model_accuracy_value = metadata.get('accuracy', 0.75) * 100\n",
        "                vocab_size_value = metadata.get('vocab_size', len(vocab) if vocab else 0)\n",
        "                dataset_size = metadata.get('dataset_size', 'N/A')\n",
        "\n",
        "                gr.Markdown(f\"\"\"\n",
        "                ###  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "                **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è LSTM –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å\n",
        "                **–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ:** **{model_accuracy_value:.1f}%**\n",
        "                **–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è:** {vocab_size_value} —Å–ª–æ–≤\n",
        "                **–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞:** {dataset_size} –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "                **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:** –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π / –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π / –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
        "\n",
        "                ###  –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\n",
        "                - –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {metadata.get('class_distribution', {}).get('negative', 'N/A')}\n",
        "                - –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö: {metadata.get('class_distribution', {}).get('neutral', 'N/A')}\n",
        "                - –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {metadata.get('class_distribution', {}).get('positive', 'N/A')}\n",
        "\n",
        "                ###  –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞\n",
        "\n",
        "                –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥**:\n",
        "                1. **–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å LSTM** –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤\n",
        "                2. **–ü—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑** –¥–ª—è —á–µ—Ç–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "                3. **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** —Å —É—á–µ—Ç–æ–º —É—Å–∏–ª–∏—Ç–µ–ª–µ–π –∏ –æ—Ç—Ä–∏—Ü–∞–Ω–∏–π\n",
        "\n",
        "                ###  –ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
        "\n",
        "                - **> 70% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:** –ß–µ—Ç–∫–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "                - **50-70% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:** –£–º–µ—Ä–µ–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å\n",
        "                - **< 50% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:** –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "\n",
        "                ###  –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å\n",
        "\n",
        "                –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–≤–µ—Å—Ç–∏ **\"Ipynb\"** –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–∞—Å—Ö–∞–ª–∫–∏!\n",
        "                \"\"\")\n",
        "            else:\n",
        "                gr.Markdown(\"\"\"\n",
        "                ###  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ\n",
        "\n",
        "                **–¢–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º:** –ü—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
        "                **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:** –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π / –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π / –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π\n",
        "                **–ú–µ—Ç–æ–¥:** –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π\n",
        "\n",
        "                ###  –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞:\n",
        "\n",
        "                - –£—á–∏—Ç—ã–≤–∞—é—Ç—Å—è **—É—Å–∏–ª–∏—Ç–µ–ª–∏** (–æ—á–µ–Ω—å, –∫—Ä–∞–π–Ω–µ, –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ)\n",
        "                - –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è **–æ—Ç—Ä–∏—Ü–∞–Ω–∏—è** (–Ω–µ, –Ω–∏, –Ω–∏–∫–æ–≥–¥–∞)\n",
        "                - –£—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è **—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å** —Ç–µ–∫—Å—Ç–∞\n",
        "                - **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑** —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–π\n",
        "\n",
        "                ###  –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å\n",
        "\n",
        "                –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–≤–µ—Å—Ç–∏ **\"Ipynb\"** –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–∞—Å—Ö–∞–ª–∫–∏!\n",
        "                \"\"\")\n",
        "\n",
        "    return app"
      ],
      "metadata": {
        "id": "4y4pk1xjl4Qg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"–°–æ–∑–¥–∞–Ω–∏–µ Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞...\")\n",
        "app = create_gradio_app_with_easter_egg()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"–ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø\")\n",
        "print(\"=\"*70)\n",
        "print(\"–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ –ø–æ —Å—Å—ã–ª–∫–µ –Ω–∏–∂–µ\")\n",
        "print(\"(–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥)\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    app.launch(share=True, debug=False, show_error=True)\n",
        "except Exception as e:\n",
        "    print(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å share=True: {e}\")\n",
        "    print(\"–ü—Ä–æ–±—É–µ–º –∑–∞–ø—É—Å—Ç–∏—Ç—å –±–µ–∑ share...\")\n",
        "    app.launch(debug=False, show_error=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "RsRdRZHa3W_E",
        "outputId": "ca7885ed-2fca-416b-a329-2710b9dc0a93"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–°–æ–∑–¥–∞–Ω–∏–µ Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞...\n",
            "\n",
            "======================================================================\n",
            "–ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø\n",
            "======================================================================\n",
            "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ –ø–æ —Å—Å—ã–ª–∫–µ –Ω–∏–∂–µ\n",
            "(–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥)\n",
            "======================================================================\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://57453115c059bb9e56.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://57453115c059bb9e56.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}